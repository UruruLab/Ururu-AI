import re
import time
import logging
from typing import List, Dict, Tuple, Optional
from abc import ABC, abstractmethod
from datetime import datetime
from sentence_transformers import SentenceTransformer

from app.core.config import settings


logger = logging.getLogger(__name__)


class TextPreprocessor:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œêµ­ì–´, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì ë§Œ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£.,!?%-]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        return text.strip()
    
    @staticmethod
    def remove_stopwords(text: str, stopwords: List[str] = None) -> str:
        """ë¶ˆìš©ì–´ ì œê±°"""
        if stopwords is None:
            stopwords = settings.KOREAN_STOPWORDS
        
        words = text.split()
        filtered_words = [word for word in words if word not in stopwords]
        return ' '.join(filtered_words)
    
    @staticmethod
    def normalize_korean_text(text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        # ì„¤ì •ì—ì„œ ì •ê·œí™” ë§¤í•‘ ê°€ì ¸ì˜¤ê¸°
        replacements = settings.BEAUTY_TERMS_MAPPING
        
        for original, normalized in replacements.items():
            text = text.replace(original, normalized)
        
        return text
    
    @classmethod
    def preprocess_product_text(cls, product_name: str, brand: str,
                              description: str, ingredients: str = None,
                              category: str = None) -> str:
        """ìƒí’ˆ ì •ë³´ë¥¼ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ë¡œ ì „ì²˜ë¦¬"""
        
        # ê° í•„ë“œë³„ ì „ì²˜ë¦¬
        clean_name = cls.clean_text(product_name)
        clean_brand = cls.clean_text(brand)
        clean_description = cls.clean_text(description)
        clean_ingredients = cls.clean_text(ingredients) if ingredients else ""
        clean_category = cls.clean_text(category) if category else ""
        
        # í•œêµ­ì–´ ì •ê·œí™”
        clean_description = cls.normalize_korean_text(clean_description)
        clean_ingredients = cls.normalize_korean_text(clean_ingredients)
        
        # í†µí•© í…ìŠ¤íŠ¸ êµ¬ì„±
        parts = []
        
        if clean_name:
            parts.append(f"ìƒí’ˆëª…: {clean_name}")
        if clean_brand:
            parts.append(f"ë¸Œëœë“œ: {clean_brand}")
        if clean_category:
            parts.append(f"ì¹´í…Œê³ ë¦¬: {clean_category}")
        if clean_description:
            parts.append(f"ì„¤ëª…: {clean_description}")
        if clean_ingredients:
            parts.append(f"ì„±ë¶„: {clean_ingredients}")
        
        combined_text = " | ".join(parts)
        
        # ìµœì¢… ì •ë¦¬
        final_text = cls.remove_stopwords(combined_text)
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ (BERT ëª¨ë¸ ì œí•œ ê³ ë ¤)
        # í•œêµ­ì–´ëŠ” í† í°í™” ì‹œ ë” ë§ì€ í† í°ì´ ìƒì„±ë  ìˆ˜ ìˆì–´ ë³´ìˆ˜ì ìœ¼ë¡œ ì œí•œ
        if len(final_text) > settings.MAX_SEQUENCE_LENGTH * 2:
            final_text = final_text[:settings.MAX_SEQUENCE_LENGTH * 2]
        
        return final_text


class EmbeddingServiceInterface(ABC):
    """ì„ë² ë”© ì„œë¹„ìŠ¤ ì¸í„°í˜ì´ìŠ¤"""
    
    @abstractmethod
    def encode_text(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        pass
    
    @abstractmethod
    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© ë³€í™˜"""
        pass
    
    @abstractmethod
    def get_model_info(self) -> Dict[str, str]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        pass


class ProductEmbeddingGenerator:
    """ìƒí’ˆ ì„ë² ë”© ìƒì„± ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, embedding_service: EmbeddingServiceInterface):
        self.embedding_service = embedding_service
        self.preprocessor = TextPreprocessor()
        
    def generate_product_embedding(self, product_name: str, brand: str,
                                 description: str, ingredients: str = None,
                                 category: str = None) -> Tuple[List[float], str]:
        """ë‹¨ì¼ ìƒí’ˆì˜ ì„ë² ë”© ìƒì„±"""
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = self.preprocessor.preprocess_product_text(
            product_name, brand, description, ingredients, category
        )
        
        logger.info(f"ìƒí’ˆ ì„ë² ë”© ìƒì„± ì¤‘: {product_name[:20]}...")
        
        # ì„ë² ë”© ìƒì„±
        try:
            embedding = self.embedding_service.encode_text(processed_text)
            logger.info(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ. ì°¨ì›: {len(embedding)}")
            return embedding, processed_text
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def generate_batch_embeddings(self, products_data: List[Dict]) -> List[Tuple[List[float], str]]:
        """ì—¬ëŸ¬ ìƒí’ˆì˜ ì„ë² ë”©ì„ ë°°ì¹˜ë¡œ ìƒì„±"""
        
        processed_texts = []
        
        # ëª¨ë“  ìƒí’ˆ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        for product in products_data:
            processed_text = self.preprocessor.preprocess_product_text(
                product.get('name', ''),
                product.get('brand', ''),
                product.get('description', ''),
                product.get('ingredients'),
                product.get('category')
            )
            processed_texts.append(processed_text)
        
        logger.info(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘: {len(processed_texts)}ê°œ ìƒí’ˆ")
        
        try:
            # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            embeddings = self.embedding_service.encode_batch(processed_texts)
            
            # ê²°ê³¼ ì¡°í•©
            results = list(zip(embeddings, processed_texts))
            
            logger.info(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(results)}ê°œ")
            return results
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise


class EmbeddingCache:
    """ì„ë² ë”© ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self._cache: Dict[str, Tuple[List[float], datetime]] = {}
        self.ttl_seconds = settings.CACHE_TTL_SECONDS
    
    def _generate_cache_key(self, text: str, model_version: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        content = f"{text}_{model_version}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, text: str, model_version: str) -> Optional[List[float]]:
        """ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ"""
        cache_key = self._generate_cache_key(text, model_version)
        
        if cache_key in self._cache:
            embedding, created_at = self._cache[cache_key]
            
            # TTL ì²´í¬
            elapsed = (datetime.now() - created_at).total_seconds()
            if elapsed < self.ttl_seconds:
                logger.debug(f"ìºì‹œ íˆíŠ¸: {cache_key[:8]}...")
                return embedding
            else:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self._cache[cache_key]
                logger.debug(f"ìºì‹œ ë§Œë£Œ: {cache_key[:8]}...")
        
        return None
    
    def set(self, text: str, model_version: str, embedding: List[float]):
        """ìºì‹œì— ì„ë² ë”© ì €ì¥"""
        cache_key = self._generate_cache_key(text, model_version)
        self._cache[cache_key] = (embedding, datetime.now())
        logger.debug(f"ìºì‹œ ì €ì¥: {cache_key[:8]}...")
    
    def clear_expired(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        current_time = datetime.now()
        expired_keys = []
        
        for key, (_, created_at) in self._cache.items():
            elapsed = (current_time - created_at).total_seconds()
            if elapsed >= self.ttl_seconds:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self._cache[key]
        
        if expired_keys:
            logger.info(f"ë§Œë£Œëœ ìºì‹œ {len(expired_keys)}ê°œ ì •ë¦¬ ì™„ë£Œ")
    
    def get_cache_stats(self) -> Dict[str, int]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        return {
            "total_items": len(self._cache),
            "ttl_seconds": self.ttl_seconds
        }


class EmbeddingService(EmbeddingServiceInterface):
    """SentenceTransformer ê¸°ë°˜ ì„ë² ë”© ì„œë¹„ìŠ¤ êµ¬í˜„ì²´"""
    
    def __init__(self):
        self.model_name = settings.EMBEDDING_MODEL_NAME
        self.model = None
        self.cache = EmbeddingCache()
        self._validate_model_config()
        self._load_model()
    
    def _validate_model_config(self):
        """ëª¨ë¸ ì„¤ì • ê²€ì¦"""
        logger.info(f"ğŸ”§ í˜„ì¬ í™˜ê²½: {settings.ENVIRONMENT}")
        logger.info(f"ğŸ“ ì„ë² ë”© ëª¨ë¸: {self.model_name}")
        logger.info(f"ğŸ“ ì„ë² ë”© ì°¨ì›: {settings.EMBEDDING_DIMENSION}")
        logger.info(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {settings.PRODUCT_EMBEDDING_BATCH_SIZE}")
        
        # ê°œë°œ í™˜ê²½ì—ì„œ ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
        if settings.is_development and "large" in self.model_name.lower():
            logger.warning("âš ï¸  ê°œë°œ í™˜ê²½ì—ì„œ ëŒ€í˜• ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ ì´ìŠˆê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ìš´ì˜ í™˜ê²½ì—ì„œ í•œêµ­ì–´ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
        if settings.is_production and "kr-" not in self.model_name.lower():
            logger.warning("âš ï¸  ìš´ì˜ í™˜ê²½ì—ì„œ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ì´ ì•„ë‹™ë‹ˆë‹¤. ì„±ëŠ¥ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            logger.info(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œì‘: {self.model_name}")
            start_time = time.time()
            
            self.model = SentenceTransformer(self.model_name)
            
            load_time = time.time() - start_time
            logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
            
            # ëª¨ë¸ ì •ë³´ ê²€ì¦
            model_dim = self.model.get_sentence_embedding_dimension()
            if model_dim != settings.EMBEDDING_DIMENSION:
                logger.error(f"âŒ ëª¨ë¸ ì°¨ì› ë¶ˆì¼ì¹˜: ì„¤ì •={settings.EMBEDDING_DIMENSION}, ì‹¤ì œ={model_dim}")
                raise ValueError(f"ëª¨ë¸ ì°¨ì›ì´ ì„¤ì •ê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {model_dim} != {settings.EMBEDDING_DIMENSION}")
                
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def encode_text(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        # ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ê³µë°±ë§Œ ìˆëŠ” í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if not text or not text.strip():
            logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì–´ ì œë¡œ ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            # í™”ì¥í’ˆ ë„ë©”ì¸ì—ì„œëŠ” í…ìŠ¤íŠ¸ ì •ë³´ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ì œë¡œ ë²¡í„° ì‚¬ìš©
            # ì´ëŠ” í•´ë‹¹ ìƒí’ˆì´ ì¶”ì²œì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì œì™¸ë˜ë„ë¡ í•¨
            return [0.0] * settings.EMBEDDING_DIMENSION
        
        # ìºì‹œ í™•ì¸
        cached_embedding = self.cache.get(text, self.model_name)
        if cached_embedding:
            return cached_embedding
        
        try:
            # ì„ë² ë”© ìƒì„±
            embedding = self.model.encode(text, convert_to_tensor=False)
            embedding_list = embedding.tolist()
            
            # ìºì‹œ ì €ì¥
            self.cache.set(text, self.model_name, embedding_list)
            
            return embedding_list
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return [0.0] * settings.EMBEDDING_DIMENSION
    
    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”© ë³€í™˜"""
        if not texts:
            return []
        
        try:
            # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
            embeddings = self.model.encode(texts, convert_to_tensor=False, batch_size=settings.PRODUCT_EMBEDDING_BATCH_SIZE)
            
            # ê°ê° ìºì‹œì— ì €ì¥
            results = []
            for text, embedding in zip(texts, embeddings):
                embedding_list = embedding.tolist()
                self.cache.set(text, self.model_name, embedding_list)
                results.append(embedding_list)
            
            return results
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
            return [self.encode_text(text) for text in texts]
    
    def get_model_info(self) -> Dict[str, str]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.model_name,
            "model_type": "SentenceTransformer",
            "embedding_dimension": str(settings.EMBEDDING_DIMENSION),
            "max_sequence_length": str(settings.MAX_SEQUENCE_LENGTH)
        }
