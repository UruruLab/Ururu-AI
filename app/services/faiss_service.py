import faiss
import numpy as np
import json
import pickle
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor

from app.core.config import settings

logger = logging.getLogger(__name__)

class FaissIndexManager:
    """Faiss ì¸ë±ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, dimension: int = settings.EMBEDDING_DIMENSION):
        self.dimension = dimension
        self.index = None
        self.proudct_ids = []
        self.metadata = {}
        self.index_type = settings.FAISS_INDEX_TYPE
        self.index_path = Path(settings.FAISS_INDEX_PATH)
        self.index_path.mkdir(parents=True, exist_ok=True)

        self.executor = ThreadPoolExecutor(max_workers=settings.FAISS_THREAD_POOL_SIZE)

        logger.debug(f"Faiss ì¸ë±ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™” - ì°¨ì›: {dimension}, ì¸ë±ìŠ¤ íƒ€ì…: {self.index_type}")

    def _create_index(self, index_type: str = None) -> faiss.Index:
        """Faiss ì¸ë±ìŠ¤ ìƒì„±"""
        if index_type is None:
            index_type = self.index_type

        if index_type == "IndexFlatIP":
            # ë‚´ì  ê¸°ë°˜ ì¸ë±ìŠ¤ - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©
            index = faiss.IndexFlatIP(self.dimension)
        elif index_type == "IndexFlatL2":
            # L2 ê±°ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤
            index = faiss.IndexFlatL2(self.dimension)
        elif index_type == "IndexIVFFlat":
            # IVF ì¸ë±ìŠ¤ - ëŒ€ìš©ëŸ‰ ë°ì´í„°ìš©
            quantizer = faiss.IndexFlatL2(self.dimension)
            index = faiss.IndexIVFFlat(quantizer, self.dimension, 100) 
        elif index_type == "IndexHNSW":
            # HNSW ì¸ë±ìŠ¤ - ë¹ ë¥¸ ê·¼ì‚¬ ê²€ìƒ‰
            index = faiss.IndexHNSWFlat(self.dimension, 32) 
        else:
            # ê¸°ë³¸ê°’: Flat IP
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì¸ë±ìŠ¤ íƒ€ì…: {index_type}, IndexFlatIP ì‚¬ìš©")
            index = faiss.IndexFlatIP(self.dimension)
        
        return index
    
    def initialize_index(self, force_recreate: bool = False):
        """Faiss ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        index_file = self.index_path / f"product_index_{self.index_type}.faiss"
        metadata_file = self.index_path / f"product_metadata_{self.index_type}.json"

        if not force_recreate and index_file.exists() and metadata_file.exists():
            try:
                self._load_index()
                logger.debug(f"ê¸°ì¡´ Faiss ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.product_ids)}ê°œ ë²¡í„°")
                return
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}, ì¸ë±ìŠ¤ ì¬ìƒì„± í•„ìš”")

        self.index = self._create_index()
        self.proudct_ids = []
        self.metadata = {
            "created_at": datetime.now().isoformat(),
            "index_type": self.index_type,
            "dimension": self.dimension,
            "total_vectors": 0
        }

        logger.debug("ìƒˆ Faiss ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

    def add_vectors(self, vectors:np.ndarray, product_ids: List[int], batch_metadate: Dict = None):
        """ë²¡í„°ë“¤ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€"""
        if self.index is None:
            raise ValueError("ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if len(vectors) != len(product_ids):
            raise ValueError("ë²¡í„°ì™€ ì œí’ˆ IDì˜ ê¸¸ì´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        if self.index_type == "IndexIVFFlat" :
            vectors = self._normalize_vectors(vectors)

        start_idx = len(self.proudct_ids)
        self.index.add(vectors.astype(np.float32))
        self.proudct_ids.extend(product_ids)

        self.metadata["total_vectors"] = len(self.proudct_ids)
        self.metadata["last_updated"] = datetime.now().isoformat()

        if batch_metadate:
            self.metadata.update(batch_metadate)

        logger.debug(f"{len(vectors)}ê°œì˜ ë²¡í„°ë¥¼ ì¸ë±ìŠ¤ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ì´ {len(self.proudct_ids)}ê°œ ë²¡í„°")

        return start_idx, start_idx + len(vectors)
    
    def search(self, query_vector: np.ndarray, k: int = 10) -> Tuple[List[float], List[int]]:
        """ë²¡í„° ê²€ìƒ‰"""
        if self.index is None or len(self.proudct_ids) == 0:
            logger.warning("ë¹ˆ ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰ ìš”ì²­")
            return [],[]
        
        if self.index_typ == "IndexFlatIP":
            query_vector = self._normalize_vectors(query_vector.reshape(1, -1))
        else:
            query_vector = query_vector.reshape(1, -1)

        k = min(k, len(self.proudct_ids))
        scores, indices = self.index.search(query_vector.astype(np.float32), k)
        scores = scores[0].tolist()
        product_ids = [self.proudct_ids[idx] for idx in indices[0] if idx < len(self.proudct_ids)]

        logger.debug(f"ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: top-{k}, ìµœê³  ì ìˆ˜: {scores[0]:.4f}")

        return scores, product_ids
    
    async def search_async(self, query_vector: np.ndarray, k: int = 10) -> Tuple[List[float], List[int]]:
        """ë¹„ë™ê¸° ë²¡í„° ê²€ìƒ‰"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, self.search, query_vector, k)
    
    def _normalize_vectors(self, vectors: np.ndarray) -> np.ndarray:
        """ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)"""
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1 
        return vectors / norms
    
    def save_index(self):
        """ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if self.index is None:
            logger.warning("ì €ì¥í•  ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        index_file = self.index_path / f"product_index_{self.index_type}.faiss"
        metadata_file = self.index_path / f"product_metadata_{self.index_type}.json"
        
        try:
            faiss.write_index(self.index, str(index_file))
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "metadata": self.metadata,
                    "product_ids": self.product_ids
                }, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ Faiss ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_file}")
            
        except Exception as e:
            logger.warning(f"ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        index_file = self.index_path / f"product_index_{self.index_type}.faiss"
        metadata_file = self.index_path / f"product_metadata_{self.index_type}.json"
        
        self.index = faiss.read_index(str(index_file))
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.metadata = data["metadata"]
            self.product_ids = data["product_ids"]
        
        logger.debug(f"ğŸ“‚ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.product_ids)}ê°œ ë²¡í„°")
    
    def remove_vectors(self, product_ids_to_remove: List[int]):
        """íŠ¹ì • ìƒí’ˆë“¤ì˜ ë²¡í„° ì œê±° (ì¬êµ¬ì¶• ë°©ì‹)"""
        if not product_ids_to_remove:
            return
        
        remaining_indices = []
        remaining_product_ids = []
        
        for i, pid in enumerate(self.product_ids):
            if pid not in product_ids_to_remove:
                remaining_indices.append(i)
                remaining_product_ids.append(pid)
        
        if not remaining_indices:
            self.index = self._create_index()
            self.product_ids = []
            logger.info("ğŸ—‘ï¸ ëª¨ë“  ë²¡í„° ì œê±° ì™„ë£Œ")
            return
        
        old_vectors = np.array([self.index.reconstruct(i) for i in remaining_indices])
        
        self.index = self._create_index()
        self.product_ids = []
        self.add_vectors(old_vectors, remaining_product_ids)
        
        logger.debug(f"ğŸ—‘ï¸ ë²¡í„° ì œê±° ì™„ë£Œ: {len(product_ids_to_remove)}ê°œ ì œê±°, {len(remaining_product_ids)}ê°œ ìœ ì§€")

    def get_index_stats(self) -> Dict:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´"""
        if self.index is None:
            return {"status": "not_initialized"}
        
        return {
            "status": "ready",
            "index_type": self.index_type,
            "dimension": self.dimension,
            "total_vectors": len(self.product_ids),
            "index_size_mb": self.index.ntotal * self.dimension * 4 / (1024 * 1024),  # float32 ê¸°ì¤€
            "metadata": self.metadata
        }
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)
        logger.info("ğŸ”’ Faiss ì¸ë±ìŠ¤ ë§¤ë‹ˆì € ì¢…ë£Œ")
        
    
class FaissVectorStore:
    """Faiss ë²¡í„° ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.index_manager = FaissIndexManager()
        self.index_manager.initialize_index()
        logger.info("ğŸš€ Faiss ë²¡í„° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def add_product_embeddings(self, embeddings_data: List[Dict]) -> bool:
        """ìƒí’ˆ ì„ë² ë”©ë“¤ì„ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€"""
        try:
            if not embeddings_data:
                logger.warning("ì¶”ê°€í•  ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            vectors = []
            product_ids = []
            
            for data in embeddings_data:
                vectors.append(data["embedding"])
                product_ids.append(data["product_id"])
            
            vectors_array = np.array(vectors)
            
            # ë¹„ë™ê¸°ì ìœ¼ë¡œ ì¶”ê°€
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                self.index_manager.executor,
                self.index_manager.add_vectors,
                vectors_array,
                product_ids
            )
            
            # ìë™ ì €ì¥
            await loop.run_in_executor(
                self.index_manager.executor,
                self.index_manager.save_index
            )
            
            logger.info(f"ìƒí’ˆ ì„ë² ë”© ì¶”ê°€ ì™„ë£Œ: {len(embeddings_data)}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ì„ë² ë”© ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    async def search_vectors(self, query_embedding: List[float], k: int = 10) -> Tuple[List[float], List[int]]:
        """ìˆœìˆ˜ ë²¡í„° ê²€ìƒ‰"""
        try:
            query_vector = np.array(query_embedding)
            scores, indices = await self.index_manager.search_raw_async(query_vector, k)
            
            # ìƒí’ˆ ID ë³€í™˜
            product_ids = self.index_manager.get_product_ids_by_indices(indices)
            
            logger.debug(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(product_ids)}ê°œ ê²°ê³¼")
            return scores.tolist(), product_ids
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return [], []
    
    def get_service_stats(self) -> Dict:
        """ì„œë¹„ìŠ¤ í†µê³„ ì •ë³´"""
        return {
            "service_name": "FaissVectorService",
            "index_stats": self.index_manager.get_index_stats(),
            "settings": {
                "embedding_dimension": settings.EMBEDDING_DIMENSION,
                "index_type": settings.FAISS_INDEX_TYPE,
                "min_similarity_threshold": settings.MIN_SIMILARITY_THRESHOLD,
                "max_similarity_threshold": settings.MAX_SIMILARITY_THRESHOLD
            }
        }
    
    async def close(self):
        """ì„œë¹„ìŠ¤ ì¢…ë£Œ"""
        if hasattr(self.index_manager, 'executor'):
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self.index_manager.close)
        logger.info("ğŸ”’ Faiss ë²¡í„° ì„œë¹„ìŠ¤ ì¢…ë£Œ")

