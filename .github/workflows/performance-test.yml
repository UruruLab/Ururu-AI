name: Performance Test

on:
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Test execution time (seconds)'
        required: true
        default: '60'
        type: string
      concurrent_users:
        description: 'Number of concurrent users'
        required: true
        default: '5'
        type: string

jobs:
  performance-test:
    runs-on: ubuntu-latest
    
    steps:
      - name: ÏΩîÎìú Ï≤¥ÌÅ¨ÏïÑÏõÉ
        uses: actions/checkout@v4

      - name: Python ÌôòÍ≤Ω ÏÑ§Ï†ï
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: ÏÑ±Îä• ÌÖåÏä§Ìä∏ ÎèÑÍµ¨ ÏÑ§Ïπò
        run: |
          pip install locust==2.31.0 requests==2.32.3 fastapi==0.111.0 uvicorn==0.30.0

      - name: ÌÖåÏä§Ìä∏Ïö© FastAPI ÏÑúÎ≤Ñ ÏãúÏûë
        run: |
          # Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏ ÏÑúÎ≤Ñ ÏÉùÏÑ±
          cat > test_server.py << 'EOF'
          from fastapi import FastAPI
          import uvicorn
          import asyncio
          
          app = FastAPI(title="Test Ururu AI Server")
          
          @app.get("/health")
          async def health_check():
              return {"status": "healthy", "service": "ururu-ai-test"}
              
          @app.post("/api/v1/recommendations")
          async def mock_recommendations():
              await asyncio.sleep(0.1)  # Ïã§Ï†ú AI Ï≤òÎ¶¨ ÏãúÍ∞Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò
              return {
                  "recommendations": [
                      {"product_id": 1, "score": 0.95, "name": "ÌÖåÏä§Ìä∏ ÏÉÅÌíà 1"},
                      {"product_id": 2, "score": 0.89, "name": "ÌÖåÏä§Ìä∏ ÏÉÅÌíà 2"}
                  ],
                  "total_count": 2,
                  "processing_time_ms": 100
              }
          EOF
          
          # Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏÑúÎ≤Ñ Ïã§Ìñâ
          python -c "
          import uvicorn
          import sys
          sys.path.append('.')
          uvicorn.run('test_server:app', host='0.0.0.0', port=8000, log_level='warning')
          " &
          
          # ÏÑúÎ≤Ñ ÏãúÏûë ÎåÄÍ∏∞
          sleep 10

      - name: ÏÑúÎ≤Ñ Ï§ÄÎπÑ ÏÉÅÌÉú ÌôïÏù∏
        run: |
          for i in {1..30}; do
            if curl -f http://localhost:8000/health; then
              echo "‚úÖ ÌÖåÏä§Ìä∏ ÏÑúÎ≤ÑÍ∞Ä Ï§ÄÎπÑÎêòÏóàÏäµÎãàÎã§."
              break
            fi
            echo "ÌÖåÏä§Ìä∏ ÏÑúÎ≤Ñ ÏãúÏûëÏùÑ Í∏∞Îã§Î¶¨Îäî Ï§ë... ($i/30)"
            sleep 2
          done

      - name: Create Locust Performance Test File
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          import json
          
          class UruruAITestUser(HttpUser):
              wait_time = between(1, 3)
              
              def on_start(self):
                  self.client.verify = False
                  
              @task(3)
              def get_recommendations(self):
                  payload = {
                      "user_diagnosis": "I have dry skin and lack moisture",
                      "top_k": 10,
                      "max_price": 50000
                  }
                  
                  with self.client.post(
                      "/api/v1/recommendations",
                      json=payload,
                      headers={"Content-Type": "application/json"},
                      catch_response=True
                  ) as response:
                      if response.status_code == 200:
                          response.success()
                      else:
                          response.failure(f"Recommendation API failed: {response.status_code}")
              
              @task(1)
              def health_check(self):
                  with self.client.get("/health", catch_response=True) as response:
                      if response.status_code == 200:
                          response.success()
                      else:
                          response.failure(f"Health check failed: {response.status_code}")
          EOF

      - name: Run Performance Test
        run: |
          echo "üöÄ Starting performance test (Users: ${{ github.event.inputs.concurrent_users }}, Time: ${{ github.event.inputs.test_duration }}s)"
          locust \
            --host=http://localhost:8000 \
            --users=${{ github.event.inputs.concurrent_users }} \
            --spawn-rate=1 \
            --run-time=${{ github.event.inputs.test_duration }}s \
            --headless \
            --csv=performance_results \
            --html=performance_report.html || echo "Performance test completed"

      - name: Analyze Performance Test Results
        run: |
          echo "=== Performance Test Results Summary ==="
          if [ -f performance_results_stats.csv ]; then
            echo "üìä Request Statistics:"
            cat performance_results_stats.csv | head -5
            echo ""
            echo "‚ùå Failure Statistics:"
            cat performance_results_failures.csv 2>/dev/null || echo "No failures"
          else
            echo "‚ö†Ô∏è Performance test result files not found."
          fi
          
          echo ""
          echo "=== Test Environment Information ==="
          echo "- Concurrent Users: ${{ github.event.inputs.concurrent_users }}"
          echo "- Test Duration: ${{ github.event.inputs.test_duration }}s"
          echo "- Server Environment: GitHub Actions (ubuntu-latest)"
          echo "- Test Target: Mock FastAPI Server"

      - name: Upload Result Files
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results-${{ github.run_number }}
          path: |
            performance_results*.csv
            performance_report.html

      - name: Cleanup Test Environment
        if: always()
        run: |
          echo "üßπ Cleaning up test environment"
          pkill -f "uvicorn" || echo "Server process cleanup completed"
          echo "‚úÖ Performance test completed!"
